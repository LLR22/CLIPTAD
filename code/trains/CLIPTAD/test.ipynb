{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/yanrui/code/CLIPBased_TAD')\n",
    "\n",
    "import random\n",
    "import os.path\n",
    "from losses.clip import clip_loss2\n",
    "from model.CLIPTAD.CLIPModel import CLIPModel\n",
    "import os\n",
    "import torch\n",
    "import heapq  # 用于管理最小堆\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import logging\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from dataset.train.wwadl import detection_collate\n",
    "from dataset.train.wwadl_multi import WWADLDatasetMutiAll\n",
    "from configs.config import config \n",
    "\n",
    "GLOBAL_SEED = 42\n",
    "\n",
    "\n",
    "def _to_var(data: dict, device):\n",
    "    for key, value in data.items():\n",
    "        data[key] = value.to(device)  # Directly move tensor to device\n",
    "    return data\n",
    "\n",
    "def worker_init_fn(worker_id):\n",
    "    np.random.seed(GLOBAL_SEED + worker_id)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class BestModelSaver:\n",
    "    def __init__(self, check_point_path, max_models=10):\n",
    "        self.check_point_path = check_point_path\n",
    "        self.max_models = max_models\n",
    "        # 使用最大堆保存模型信息 [(负的metric, model_path), ...]\n",
    "        self.best_models = []\n",
    "\n",
    "    def save_model(self, model_state_dict, model_name, metric, is_save=False):\n",
    "        # 构造保存路径\n",
    "        model_path = os.path.join(self.check_point_path, f\"{model_name}.pt\")\n",
    "\n",
    "        if is_save:\n",
    "            torch.save(model_state_dict, model_path)\n",
    "            return\n",
    "\n",
    "        # 如果队列未满，直接保存模型\n",
    "        if len(self.best_models) < self.max_models:\n",
    "            torch.save(model_state_dict, model_path)\n",
    "            # 保存负的metric以构造最大堆\n",
    "            heapq.heappush(self.best_models, (-metric, model_path))\n",
    "            print(f\"Model saved: {model_path} (Metric: {metric:.5f})\")\n",
    "        else:\n",
    "            # 检查是否优于当前最差模型（堆顶是负的最大值，对应正的最小值）\n",
    "            if metric < -self.best_models[0][0]:  # 假设指标是损失，越小越好\n",
    "                # 删除最差模型\n",
    "                _, worst_model_path = heapq.heappop(self.best_models)\n",
    "                if os.path.exists(worst_model_path):\n",
    "                    os.remove(worst_model_path)\n",
    "                    print(f\"Old model removed: {worst_model_path}\")\n",
    "\n",
    "                # 保存新模型\n",
    "                torch.save(model_state_dict, model_path)\n",
    "                heapq.heappush(self.best_models, (-metric, model_path))\n",
    "                print(f\"Model saved: {model_path} (Metric: {metric:.5f})\")\n",
    "            else:\n",
    "                print(f\"Model not saved. Metric: {metric:.5f} is worse than the top 10.\")\n",
    "\n",
    "    def get_best_models(self):\n",
    "        # 返回按指标从小到大排序的模型列表（还原负的metric）\n",
    "        return sorted([(-metric, path) for metric, path in self.best_models], key=lambda x: x[0])\n",
    "\n",
    "class Trainer_ForCLIP(object):\n",
    "    def __init__(self,\n",
    "                 train_dataset,\n",
    "                 model\n",
    "                 ):\n",
    "        super(Trainer_ForCLIP, self).__init__()\n",
    "        self.model = model\n",
    "        self.train_dataset = train_dataset\n",
    "\n",
    "        self.batch_size = 4\n",
    "        self.num_epoch = 300\n",
    "        self.lr_rate_adjust_epoch = 30\n",
    "        self.lr_rate_adjust_factor = 0.5\n",
    "        # loss setting -----------------------------------------------------------\n",
    "        self.loss = clip_loss2\n",
    "\n",
    "        # learning config ---------------------------------------------------------\n",
    "        self.opt_method = 'adamw'\n",
    "        self.lr_rate = 4e-4\n",
    "        self.weight_decay = 1e-3\n",
    "\n",
    "        self.check_point_path = ' /home/yanrui/code/CLIPBased_TAD/model/CLIPTAD/preTrain_Emb/'\n",
    "\n",
    "        self.model_info = 'clipEmbedding'\n",
    "        self.writer = SummaryWriter(os.path.join(self.check_point_path, f'tb_{self.model_info}'))\n",
    "\n",
    "        # DDP setting -------------------------------------------------------------\n",
    "        self.dist_url = 'env://'\n",
    "        self.rank = 0\n",
    "        self.world_size = 0\n",
    "        self.gpu=0\n",
    "\n",
    "        self.device = 'cuda'\n",
    "    \n",
    "    def _init_optimizer(self):\n",
    "\n",
    "        params = self.model.parameters()\n",
    "\n",
    "        if self.opt_method == 'adam':\n",
    "            self.optimizer = torch.optim.Adam(params=params,\n",
    "                                              lr=self.lr_rate,\n",
    "                                              weight_decay=self.weight_decay)\n",
    "        elif self.opt_method == 'adamw':\n",
    "            self.optimizer = torch.optim.AdamW(params=params,\n",
    "                                               lr=self.lr_rate,\n",
    "                                               weight_decay=self.weight_decay)\n",
    "        else:\n",
    "            self.optimizer = torch.optim.SGD(params=params,\n",
    "                                             lr=self.lr_rate,\n",
    "                                             weight_decay=self.weight_decay)\n",
    "\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer,\n",
    "                                                         self.lr_rate_adjust_epoch,\n",
    "                                                         self.lr_rate_adjust_factor)\n",
    "\n",
    "    def _train_one_step(self, data, targets):\n",
    "\n",
    "        data = _to_var(data, self.device) # 将本来是字典的数据移动到设备上\n",
    "\n",
    "        # data = data.to(self.device)  # 确保输入数据在正确的设备上\n",
    "        targets = [t.to(self.device) for t in targets] # 将label移动到设备上\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        try:\n",
    "            sig_emb, text_emb = self.model(data, targets)\n",
    "            tmp_sig_emb, tmp_text_emb = sig_emb, text_emb\n",
    "            \n",
    "        except AssertionError as e:\n",
    "            logging.info(\"Error occurred during training, saving data and model parameters for debugging...\")\n",
    "\n",
    "            # 创建保存路径\n",
    "            error_save_path = os.path.join(self.check_point_path, \"debug\")\n",
    "            os.makedirs(error_save_path, exist_ok=True)\n",
    "\n",
    "            # 保存导致问题的输入数据\n",
    "            data_save_path = os.path.join(error_save_path, \"error_data.pt\")\n",
    "            torch.save(data, data_save_path)\n",
    "            logging.info(f\"Input data saved to: {data_save_path}\")\n",
    "\n",
    "            # 保存模型参数\n",
    "            model_save_path = os.path.join(error_save_path, \"error_model.pth\")\n",
    "            torch.save(self.model.state_dict(), model_save_path)\n",
    "            logging.info(f\"Model parameters saved to: {model_save_path}\")\n",
    "\n",
    "            # 再次抛出异常以中断训练\n",
    "            raise e\n",
    "\n",
    "        loss = self.loss(tmp_sig_emb, tmp_text_emb)\n",
    "\n",
    "        # 反向传播\n",
    "        loss.backward(retain_graph=True)\n",
    "\n",
    "        # 优化器更新权重\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # 无需分布式同步，直接返回损失\n",
    "        return loss.item()\n",
    "    \n",
    "    def set_seed(self, seed):\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        torch.backends.cudnn.deterministic =True\n",
    "\n",
    "    def training(self):\n",
    "        # 给不同的进程分配不同的、固定的随机数种子\n",
    "        self.set_seed(2024)\n",
    "        # register_hooks(self.model) # 钩子函数？不懂，后面去研究\n",
    "        device = torch.device(self.device)\n",
    "\n",
    "        # dataset loader -------------------------------------------------------------------------------\n",
    "        nw = min([os.cpu_count(), self.batch_size if self.batch_size > 1 else 0, 8])  # number of workers\n",
    "        print('Using {} dataloader workers every process'.format(nw))\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,  # DataParallel 不需要使用分布式采样\n",
    "            pin_memory=True,\n",
    "            num_workers=nw,  # 动态设置 workers\n",
    "            collate_fn=detection_collate,  # 自定义 collate_fn\n",
    "            worker_init_fn=worker_init_fn,  # 初始化每个 worker 的随机种子\n",
    "            drop_last = True\n",
    "        )\n",
    "\n",
    "        # load model ------------------------------------------------------------------------------------\n",
    "\n",
    "        # 转为DataParallel模型 ---------------------------------------------------------------------------\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            print(f\"Using {torch.cuda.device_count()} GPUs for training\")\n",
    "            self.model = torch.nn.DataParallel(self.model, device_ids=list(range(torch.cuda.device_count())))\n",
    "        else:\n",
    "            print(\"Using a single GPU for training\")\n",
    "\n",
    "        self.model = self.model.to(device=device)\n",
    "\n",
    "        self._init_optimizer()\n",
    "\n",
    "        mini_train_loss = float('inf')\n",
    "        saver = BestModelSaver(self.check_point_path, max_models=1)  # 初始化最佳模型管理\n",
    "\n",
    "        for epoch in range(self.num_epoch):\n",
    "            np.random.seed(epoch)  # 设置随机种子\n",
    "            self.model.train()\n",
    "\n",
    "            tbar = tqdm(train_loader)\n",
    "\n",
    "            iteration = 0\n",
    "            cost_val = 0\n",
    "            loss_val = 0\n",
    "\n",
    "            for clips, targets in tbar:\n",
    "                iteration += 1\n",
    "                \n",
    "                loss = self._train_one_step(clips, targets)\n",
    "\n",
    "                loss_val += loss\n",
    "                cost_val += loss\n",
    "\n",
    "                tbar.set_description('Epoch: %d: ' % (epoch + 1))\n",
    "                tbar.set_postfix(train_loss=loss)\n",
    "                # 每次迭代清理显存\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "            tbar.close()\n",
    "\n",
    "            loss_val /= (iteration + 1)\n",
    "            cost_val /= (iteration + 1)\n",
    "            plog = 'Epoch-{} Loss: Total - {:.5f}, loc - {:.5f}' \\\n",
    "                .format(epoch, cost_val, loss_val)\n",
    "\n",
    "            logging.info(plog)\n",
    "\n",
    "            self.scheduler.step()\n",
    "\n",
    "            # 保存当前模型\n",
    "            saver.save_model(self.model.state_dict(), f\"{self.model_info}-epoch-{epoch}\", cost_val)\n",
    "\n",
    "            self.writer.add_scalar(\"Train Loss\", cost_val, epoch)\n",
    "            self.writer.add_scalar(\"loss_val Loss\", loss_val, epoch)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安全加载函数\n",
    "def safe_load(model, pretrained_path):\n",
    "    pretrained_dict = torch.load(pretrained_path)\n",
    "    model_dict = model.state_dict()\n",
    "    \n",
    "    # 1. 过滤不匹配的键\n",
    "    pretrained_dict = {k: v for k, v in pretrained_dict.items() \n",
    "                      if k in model_dict and v.shape == model_dict[k].shape}\n",
    "    \n",
    "    # 2. 严格检查版本\n",
    "    for k, v in pretrained_dict.items():\n",
    "        if model_dict[k].requires_grad:\n",
    "            v.requires_grad_(True)  # 保持梯度追踪一致性\n",
    "    \n",
    "    model.load_state_dict(pretrained_dict, strict=False)  # 非严格模式\n",
    "    print(f\"Loaded {len(pretrained_dict)}/{len(model_dict)} parameters\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 467/467 parameters\n",
      "Using 4 dataloader workers every process\n",
      "Using a single GPU for training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2433 [00:00<?, ?it/s]/home/yanrui/anaconda3/envs/py38/lib/python3.8/site-packages/torch/autograd/__init__.py:197: UserWarning: Error detected in MmBackward0. Traceback of forward call that caused the error:\n",
      "  File \"/home/yanrui/anaconda3/envs/py38/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/yanrui/anaconda3/envs/py38/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/yanrui/anaconda3/envs/py38/lib/python3.8/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/yanrui/anaconda3/envs/py38/lib/python3.8/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/yanrui/anaconda3/envs/py38/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 619, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/yanrui/anaconda3/envs/py38/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/yanrui/anaconda3/envs/py38/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/yanrui/anaconda3/envs/py38/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/yanrui/anaconda3/envs/py38/lib/python3.8/asyncio/events.py\", line 81, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/yanrui/anaconda3/envs/py38/lib/python3.8/site-packages/tornado/ioloop.py\", line 688, in <lambda>\n",
      "    lambda f: self._run_callback(functools.partial(callback, future))\n",
      "  File \"/home/yanrui/anaconda3/envs/py38/lib/python3.8/site-packages/tornado/ioloop.py\", line 741, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/home/yanrui/anaconda3/envs/py38/lib/python3.8/site-packages/tornado/gen.py\", line 814, in inner\n",
      "    self.ctx_run(self.run)\n",
      "  File \"/home/yanrui/anaconda3/envs/py38/lib/python3.8/site-packages/tornado/gen.py\", line 775, in run\n",
      "    yielded = self.gen.send(value)\n",
      "  File \"/home/yanrui/anaconda3/envs/py38/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 358, in process_one\n",
      "    yield gen.maybe_future(dispatch(*args))\n",
      "  File \"/home/yanrui/anaconda3/envs/py38/lib/python3.8/site-packages/tornado/gen.py\", line 234, in wrapper\n",
      "    yielded = ctx_run(next, result)\n",
      "  File \"/home/yanrui/anaconda3/envs/py38/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
      "    yield gen.maybe_future(handler(stream, idents, msg))\n",
      "  File \"/home/yanrui/anaconda3/envs/py38/lib/python3.8/site-packages/tornado/gen.py\", line 234, in wrapper\n",
      "    yielded = ctx_run(next, result)\n",
      "  File \"/home/yanrui/anaconda3/envs/py38/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 536, in execute_request\n",
      "    self.do_execute(\n",
      "  File \"/home/yanrui/anaconda3/envs/py38/lib/python3.8/site-packages/tornado/gen.py\", line 234, in wrapper\n",
      "    yielded = ctx_run(next, result)\n",
      "  File \"/home/yanrui/anaconda3/envs/py38/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/yanrui/anaconda3/envs/py38/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/home/yanrui/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2974, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/yanrui/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3029, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/home/yanrui/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/yanrui/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3256, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/yanrui/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3472, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/home/yanrui/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3552, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-7-854259ed86e5>\", line 8, in <cell line: 4>\n",
      "    model = CLIPModel().cuda()\n",
      "  File \"/home/yanrui/code/CLIPBased_TAD/model/CLIPTAD/CLIPModel.py\", line 18, in __init__\n",
      "    self.embedding_text = TextEmbedding2(device=self.device)\n",
      "  File \"/home/yanrui/code/CLIPBased_TAD/model/embeddings.py\", line 44, in __init__\n",
      "    'end': self._precompute_embeddings(id_to_attribute_end)\n",
      "  File \"/home/yanrui/code/CLIPBased_TAD/model/embeddings.py\", line 57, in _precompute_embeddings\n",
      "    text_embeds = self.clip_encoder.get_text_features(**text_input)\n",
      "  File \"/home/yanrui/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/clip/modeling_clip.py\", line 1263, in get_text_features\n",
      "    text_features = self.text_projection(pooled_output)\n",
      "  File \"/home/yanrui/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/yanrui/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"/home/yanrui/anaconda3/envs/py38/lib/python3.8/site-packages/torch/fx/traceback.py\", line 57, in format_stack\n",
      "    return traceback.format_stack()\n",
      " (Triggered internally at ../torch/csrc/autograd/python_anomaly_mode.cpp:114.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "  0%|          | 0/2433 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [512, 512]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-854259ed86e5>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer_ForCLIP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset_multi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-b6e927f1e284>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    228\u001b[0m                 \u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_one_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclips\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0mloss_val\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-b6e927f1e284>\u001b[0m in \u001b[0;36m_train_one_step\u001b[0;34m(self, data, targets)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;31m# 反向传播\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;31m# 优化器更新权重\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[0;32m--> 488\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [512, 512]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"
     ]
    }
   ],
   "source": [
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0' \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #-----import data\n",
    "    train_dataset_multi = WWADLDatasetMutiAll(config['path']['train_data_path'], split='train')\n",
    "    #-----import model\n",
    "    model = CLIPModel().cuda()\n",
    "    model = safe_load(model, '/home/yanrui/code/CLIPBased_TAD/model/CLIPTAD/preTrain_Emb/clip_model_state_dict_epoch_200.pth')\n",
    "    #-----train\n",
    "    trainer = Trainer_ForCLIP(train_dataset_multi, model)\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    trainer.training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yanrui/anaconda3/envs/py38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/yanrui/code/CLIPBased_TAD')\n",
    "\n",
    "import torch\n",
    "from configs.config import config \n",
    "from model.CLIPTAD.CLIPTAD import CLIPTAD\n",
    "from model.models import RFCLIP \n",
    "from dataset.train.wwadl_multi import WWADLDatasetMutiAll\n",
    "from trains.train import Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got multiple values for argument 'isTrainClip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-4fea8c5c70ec>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtrain_dataset_multi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWWADLDatasetMutiAll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_data_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#------import model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRFCLIP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;31m#------train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset_multi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/CLIPBased_TAD/model/models.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m#----------embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'embedding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'CLIP'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mCLIPEmbed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCLIPModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misTrainClip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clip_embed_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mCLIPEmbed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got multiple values for argument 'isTrainClip'"
     ]
    }
   ],
   "source": [
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1' \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #------import data\n",
    "    train_dataset_multi = WWADLDatasetMutiAll(config['path']['train_data_path'], split='train')\n",
    "    #------import model\n",
    "    model = RFCLIP(config)\n",
    "    #------train\n",
    "    trainer = Trainer(config, train_dataset_multi, model)\n",
    "    trainer.training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/yanrui/code/WiFi_TAD/WiFiTAD_main')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
